{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe MS Teams meetings to text file\n",
    "\n",
    "As the teams meetings are <strong>huge</strong> I recommend using some external tool for extracting the audio file and just placing that on the <i>jupyter notebook</i> folder.\n",
    "\n",
    "One option is https://cloudconvert.com/mpeg-to-wav\n",
    "\n",
    "If you only need a <code>wav</code> from <code>mp3</code>, you can use https://cloudconvert.com/mp3-to-wav\n",
    "\n",
    "Of course, if you have the possibility to have the original MS Teams recording (mpgeg4) you can use ffmpeg:<BR>\n",
    "<code>ffmpeg -i <teams_recording.mp4> <output_file_name.mp3></code>\n",
    "\n",
    "\n",
    "I am very fed up with Anaconda, so the virtual environment is set with pip.<br>\n",
    "For torch, install it with instructions from https://pytorch.org/get-started/locally/<br>\n",
    "For whisper, be sure to install the openai version<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment # Pydub requires that ffmpeg is installed and in the path\n",
    "from pydub.playback import play\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper # Be sure to install openai version: pip install openai-whisper\n",
    "\n",
    "import torch, torchaudio\n",
    "\n",
    "import io\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step just checks the audio file, that it is readable and plays the first 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file to be transcripted\n",
    "file_path = 'haastattelu.mp3'\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "# Resample the audio to 16kHz (required by the model)\n",
    "audio = audio.set_frame_rate(16000)\n",
    "\n",
    "# Slice the first 10 seconds (10,000 milliseconds)\n",
    "audio_10_seconds = audio[:10000]\n",
    "\n",
    "# Save the slice as wav to see if it works\n",
    "audio_10_seconds.export('first_10_seconds.wav', format='wav')\n",
    "\n",
    "audio_file = 'first_10_seconds.wav'\n",
    "\n",
    "# PLay the first 10 secs\n",
    "display(Audio(audio_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a generator function to slice the audio file to 20 second chunks; Whisper model has been trained on 20s bits and should perform best on these inputs. Helps of course the memory, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_stream():\n",
    "    i = 0\n",
    "    chunk_size = 20000  # 20,000 milliseconds\n",
    "    while i < len(audio):\n",
    "        chunk = audio[i:i+chunk_size]\n",
    "        chunk.export('chunk.wav', format='wav')\n",
    "        i += chunk_size\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the OpenAI \"large\" model, as it seems to perform quite well on finnish. There are fine-tuned versions in finnish, but they seem to require special torch versions not available without nvidia developer accounts.\n",
    "\n",
    "Using CUDA and GPU makes inferring at least 2.5 times faster. I think there should be <strong>really</strong> good reason to not using CUDA; If you get errors with GPU RAM limit overflowing, I suggest reducing chunk size before resorting to CPU instead of GPU. An hour of Teams recording takes 23 minutes with a <strong>fast</strong> CPU but only 7 minustes with nvidia gtx 4080 with 16 VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"large\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunks are iterated through the transcription and appended to a text file.\n",
    "\n",
    "Note that the model is forced to finnish language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline and initialize it\n",
    "from auth_tokens import get_pyannotetoken\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=get_pyannotetoken()).to(torch.device(\"cuda\"))\n",
    "\n",
    "verbose = True\n",
    "with open(\"combined_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk_file in audio_stream():\n",
    "        print(f\"Current progress: {chunk_file / len(audio) * 100:.2f}%\")\n",
    "\n",
    "        # Transcribing the audio chunk\n",
    "        transcription = model.transcribe(\"chunk.wav\", language=\"fi\", verbose=verbose)\n",
    "\n",
    "        # Load the audio chunk for diarization\n",
    "        chunk_audio_path = 'chunk.wav'  # Assuming you already have this from the audio_stream\n",
    "        waveform, sample_rate = torchaudio.load(chunk_audio_path)\n",
    "        diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})\n",
    "\n",
    "        # Iterate over each segment from Whisper and find corresponding speaker\n",
    "        for segment in transcription['segments']:\n",
    "            start = segment['start']\n",
    "            end = segment['end']\n",
    "            text = segment['text']\n",
    "\n",
    "            # Find the matching speaker from diarization results\n",
    "            speaker_label = None\n",
    "            for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "                if turn.start <= start <= turn.end or \\\n",
    "                   turn.start <= end <= turn.end:\n",
    "                    speaker_label = speaker\n",
    "                    break\n",
    "\n",
    "            # Write the combined result to file\n",
    "            if speaker_label:\n",
    "                f.write(f\"Speaker[{speaker_label}] {start:.1f}-{end:.1f}: {text}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Speaker[Unknown] {start:.1f}-{end:.1f}: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load groundtruth\n",
    "from pyannote.database.util import load_rttm\n",
    "_, groundtruth = load_rttm('audio.rttm').popitem()\n",
    "\n",
    "# visualize groundtruth\n",
    "groundtruth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
