{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe MS Teams meetings to text file\n",
    "\n",
    "As the teams meetings are <strong>huge</strong> I recommend using some external tool for extracting the audio file and just placing that on the <i>jupyter notebook</i> folder.\n",
    "\n",
    "One option is https://cloudconvert.com/mpeg-to-wav\n",
    "\n",
    "If you only need a <code>wav</code> from <code>mp3</code>, you can use https://cloudconvert.com/mp3-to-wav\n",
    "\n",
    "Of course, if you have the possibility to have the original MS Teams recording (mpgeg4) you can use ffmpeg:<BR>\n",
    "<code>ffmpeg -i <teams_recording.mp4> <output_file_name.mp3></code>\n",
    "\n",
    "\n",
    "I am very fed up with Anaconda, so the virtual environment is set with pip.<br>\n",
    "For torch, install it with instructions from https://pytorch.org/get-started/locally/<br>\n",
    "For whisper, be sure to install the openai version<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment # Pydub requires that ffmpeg is installed and in the path\n",
    "from pydub.playback import play\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper # Be sure to install openai version: pip install openai-whisper\n",
    "\n",
    "import torch, torchaudio\n",
    "\n",
    "import io\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step just checks the audio file, that it is readable and plays the first 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file to be transcripted\n",
    "file_path = 'haastattelu.mp3'\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "# Resample the audio to 16kHz (required by the model)\n",
    "audio = audio.set_frame_rate(16000)\n",
    "\n",
    "# Slice the first 10 seconds (10,000 milliseconds)\n",
    "audio_10_seconds = audio[:10000]\n",
    "\n",
    "# Save the slice as wav to see if it works\n",
    "audio_10_seconds.export('first_10_seconds.wav', format='wav')\n",
    "\n",
    "audio_file = 'first_10_seconds.wav'\n",
    "\n",
    "# PLay the first 10 secs\n",
    "display(Audio(audio_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a generator function to slice the audio file to 20 second chunks; Whisper model has been trained on 20s bits and should perform best on these inputs. Helps of course the memory, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_stream():\n",
    "    i = 0\n",
    "    chunk_size = 20000  # 20,000 milliseconds\n",
    "    while i < len(audio):\n",
    "        chunk = audio[i:i+chunk_size]\n",
    "        chunk.export('chunk.wav', format='wav')\n",
    "        i += chunk_size\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the OpenAI \"large\" model, as it seems to perform quite well on finnish. There are fine-tuned versions in finnish, but they seem to require special torch versions not available without nvidia developer accounts.\n",
    "\n",
    "Using CUDA and GPU makes inferring at least 2.5 times faster. I think there should be <strong>really</strong> good reason to not using CUDA; If you get errors with GPU RAM limit overflowing, I suggest reducing chunk size before resorting to CPU instead of GPU. An hour of Teams recording takes 23 minutes with a <strong>fast</strong> CPU but only 7 minustes with nvidia gtx 4080 with 16 VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"large\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunks are iterated through the transcription and appended to a text file.\n",
    "\n",
    "Note that the model is forced to finnish language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk_file in audio_stream():\n",
    "        print(f\"Current progress: {chunk_file / len(audio) * 100:.2f}%\")\n",
    "        transcription = model.transcribe(\n",
    "            \"chunk.wav\",\n",
    "            language=\"fi\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if chunk_file == 20000:\n",
    "            print(f\"transcription at 20000: {transcription}\")\n",
    "        f.write(f\"{transcription['segments'][0]['start']:.1f}-{transcription['segments'][0]['end']:.1f} {transcription['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saku\\.cache\\torch\\pyannote\\models--pyannote--speaker-diarization-3.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:43: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  backend = torchaudio.get_audio_backend()\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:53: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(backend)\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\pyannote\\audio\\tasks\\segmentation\\mixins.py:37: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saku\\.cache\\torch\\pyannote\\models--pyannote--segmentation-3.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saku\\.cache\\torch\\pyannote\\models--pyannote--wespeaker-voxceleb-resnet34-LM. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`device` must be an instance of `torch.device`, got `str`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyannote/speaker-diarization-3.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m  \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_SqUdjRnvsBzkwcwEezRjsduThUrAlHImOd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# run the pipeline on an audio file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m diarization \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaastattelu.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saku\\Documents\\Ohjelmointi\\Python\\finnish_teams_transcription\\.venv\\lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:331\u001b[0m, in \u001b[0;36mPipeline.to\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send pipeline to `device`\"\"\"\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`device` must be an instance of `torch.device`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(device)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m     )\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, pipeline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipelines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(pipeline, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: `device` must be an instance of `torch.device`, got `str`"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=\"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\").to(torch.device(\"cuda\"))\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "diarization = pipeline(\"haastattelu.mp3\")\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(\"audio.wav\")\n",
    "diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})\n",
    "\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "with ProgressHook() as hook:\n",
    "    diarization = pipeline(\"audio.wav\", hook=hook)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
